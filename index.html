<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="GazeHELL: Gaze Estimation with Hybrid Encoders and Localised Losses with weighing, Published at BMVC 2024.">
  <meta property="og:title" content="GazeHELL"/>
  <meta property="og:description" content="GazeHELL: Gaze Estimation with Hybrid Encoders and Localised Losses with weighing, Published at BMVC 2024."/>
  <meta property="og:url" content="gazehell.github.io"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="keywords" content="Gaze Tracking, Deep Learning, Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GazeHELL: Gaze Estimation with Hybrid Encoders and Localised Losses with weighing</title>
  <link href="[https://fonts.googleapis.com/css?family=Google+Sans](https://fonts.googleapis.com/css?family=Google+Sans)|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="[https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css](https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css)">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="[https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js](https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js)"></script>
  <script src="[https://documentcloud.adobe.com/view-sdk/main.js](https://documentcloud.adobe.com/view-sdk/main.js)"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GazeHELL: Gaze Estimation with Hybrid Encoders and Localised Losses with weighing</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="[https://shubham1810.github.io/](https://shubham1810.github.io/)" target="_blank">Shubham Dokania</a><sup>1</sup>,</span>
              <span class="author-block">Vasudev Singh<sup>1</sup>,</span>
              <span class="author-block">Shuaib Ahmed<sup>1</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Merdeces-Benz Research & Development India <br>Published at BMVC 2024</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="/static/pdfs/EyeGaze__BMVC__24_.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <img src="static/images/architecture_01.png" alt="GAZEHELL: GAZE ESTIMATION" width="80%"/>
        <h2 class="subtitle has-text-centered">
          An outline of the proposed approach towards eye gaze estimation. The three main components comprise of (A) An image reconstruction and denoising branch, (B) The gaze estimation regression branch, and (C) An auxiliary uncertainty branch.
          Finally, in (D) the supervision comes from a set of losses weighed by the predicted uncertanity.
        </h2>
      </div>
    </div>
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                In the pursuit of robust eye gaze estimation, traditional approaches often grapple with the limitations of either spatial granularity or model interpretability. [cite: 1]
                This paper introduces a dual-architecture framework that synergizes the strengths of Vision Transformers (ViT) and convolutional networks to enhance gaze estimation accuracy and reliability. [cite: 2]
                We also propose two novel loss functions to refine our predictions: (1) a differentiable heatmap-based 2D MSE loss that transforms gaze vectors into a spatial heatmap enhancing the model's ability to localize gaze with high precision, and (2) a Fourier encoding loss that leverages high-dimensional Fourier features to capture complex spatial relationships more effectively. [cite: 3]
                Additionally, we incorporate auxiliary uncertainty-based task weighing into our losses to provide a measure of confidence alongside gaze estimates, aiming to improve predictions dynamically during training. [cite: 4]
                Our experimental results on MPIIGaze and RT-GENE datasets demonstrate significant improvements over existing methods and establishes a new state-of-the-art benchmark on both, with upto 3% improvement in the respective datasets. [cite: 5]
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section hero is-dark">
      <div class="container is-max-desktop">
        <div class="columns ">
          <div class="column is-four-fifths">
            <h2 class="title is-3">GazeHELL</h2>
            <div class="content has-text-justified">
              <p>
                GazeHELL is a cutting-edge architecture for gaze estimation that seamlessly integrates Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs). This hybrid approach is further enhanced by the incorporation of innovative loss functions, including a differentiable Gaussian heatmap loss and a Fourier encoding-based loss. These losses are dynamically weighted during training using an auxiliary uncertainty-based mechanism. [cite: 26]
              </p>
              <h3 class="title is-4">Key Features:</h3>
              <ul>
                <li>Dual Hybrid Encoder Architecture: Combining ViTs and CNNs to leverage their respective strengths in capturing global and local image features. [cite: 23]
                </li>
                <li>Heatmap Loss: A novel differentiable Gaussian heatmap loss for precise gaze localization. [cite: 24]
                </li>
                <li>Fourier Encoding Loss: A Fourier encoding-based loss to capture high-frequency components, enhancing sensitivity to subtle gaze variations. [cite: 25]
                </li>
                <li>Uncertainty-Based Loss Weighting: An auxiliary network dynamically adjusts loss weights, improving model robustness and accuracy. [cite: 26]
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="container is-max-desktop">
        <h2 class="title is-3">Qualitative Results</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item has-text-centered">
            <img src="static/images/gaze_01.png" alt="Input Eye Image"/>
            <h2 class="subtitle has-text-centered">
              Input Eye Image
            </h2>
          </div>
          <div class="item has-text-centered">
            <img src="static/images/gaze_02.png" alt="Ground Truth Gaze Vector"/>
            <h2 class="subtitle has-text-centered">
              Ground Truth Gaze Vector
            </h2>
          </div>
          <div class="item has-text-centered">
            <img src="static/images/gaze_03.png" alt="Gaze Prediction"/>
            <h2 class="subtitle has-text-centered">
              Gaze Prediction
            </h2>
          </div>
        </div>
      </div>
    </section>
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@article{dokania2024gazehell,
  title={GazeHELL: Gaze Estimation with Hybrid Encoders and Localised Losses with weighing},
  author={Dokania, Shubham and Singh, Vasudev and Ahmed, Shuaib},
  journal={arXiv preprint arXiv:2409.06865},
  year={2024}
}
        </code></pre>
      </div>
    </section>
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="content">
              <p>
                This page was built using the <a href="[https://github.com/eliahuhorwitz/Academic-project-page-template](https://github.com/eliahuhorwitz/Academic-project-page-template)" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="[https://nerfies.github.io](https://nerfies.github.io)" target="_blank">Nerfies</a> project page. This website is licensed under a <a rel="license" href="[http://creativecommons.org/licenses/by-sa/4.0/](http://creativecommons.org/licenses/by-sa/4.0/)" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
</body>
</html>